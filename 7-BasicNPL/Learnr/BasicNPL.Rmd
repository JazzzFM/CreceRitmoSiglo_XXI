---
title: "Analisis de textos"
output: learnr::tutorial
runtime: shiny_prerendered
tutorial:
  id: morant-clase-5
  version: 1
resource_files:
- renv.lock
- renv.lock
- renv.lock
- renv.lock
- renv.lock
- renv.lock
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(learnrhash)
library(tidyverse)
```

<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

## Introducción

En esta sección vamos a utilizar **quanteda**, que es un paquete R para administrar y analizar datos textuales. El paquete está diseñado para los usuarios de R que necesitan aplicar el procesamiento de lenguaje natural a los textos, desde los documentos hasta el análisis final. 

Sus capacidades coinciden o exceden las proporcionadas en muchas aplicaciones de software de usuario final, muchas de las cuales son caras y no están abiertas. Por lo tanto, el paquete es un gran beneficio para los investigadores, los estudiantes y otros analistas con menos recursos financieros. Mientras que el uso de Quanteda requiere conocimiento de la programación en R, su API está diseñada para permitir un análisis potente y eficiente con un mínimo de pasos. Al enfatizar el diseño constante, además, **quanteta** reduce las barreras para aprender y utilizar el análisis de texto NLP y cuantitativo.

### Instalación

Normalmente este paquete lo podemos descargar desde el CRAN, usando su GUI de R o

```{r, eval=FALSE, echo=TRUE, fig.align='center'}
install.packages("quanteda")
```

O para la última versión de desarrollo:

```{r, eval=FALSE, echo=TRUE, fig.align='center'}
# DevTools requerido para instalar Quanteda desde GitHub
devtools::install_github("quanteda/cuantda")
```

### Paquetes adicionales recomendados:

Los siguientes paquetes funcionan bien con con quanteda o lo complementan y por eso recomendamos que también los instales:

  * **readtext**: una manera sencilla de leer data de texto casi con cualquier formato con R,.

  * **spacyr**: NLP usando la librería spaCy, incluyendo etiquetado part-of-speech, entity recognition y dependency parsing.

  * **quanteda.corpora**: data textual adicional para uso con quanteda.
  
```{r, eval=FALSE, echo=TRUE, fig.align='center'}
devtools::install_github("quanteda/quanteda.corpora")
```

  * **LIWCalike**: una implementación en R del abordaje de análisis de texto Linguistic Inquiry and Word Count.

```{r, eval=FALSE, echo=TRUE, fig.align='center'}
devtools::install_github("kbenoit/quanteda.dictionaries")
```

Debido a que esto compila un código fuente de C ++ y FORTRAN, deberás haber instalado los compiladores apropiados para construir la versión de desarrollo.

**quanteda** tiene un simple y poderoso paquete adicional para cargar textos: `readtext`.

La función principal en este paquete, `readtext()`, toma un archivo o set de archivos de un disco o una dirección de URL y devuelve un tipo de data.frame que puede ser usado directamente con la función de construcción de corpus (`corpus()`) para crear un objeto corpus en quanteda. `readtext()` funciona con:

  * archivos de texto (.txt);
  * archivos de valores separados por comas (.csv);
  * data en formato XML;
  * data del API de Facebook API, en formato JSON;
  * data de la API de Twitter, en formato JSON; y
  * data en formato JSON en general.

El comando constructor de corpus llamado `corpus()` funciona directamente sobre:

  * Un vector de objetos de tipo character, por ejemplo aquellos que ya has cargado al workspace usando otras herramientas;
  * Un objeto corpus `VCorpus` del paquete `tm`.
  * Un data.frame que contenga una columna de texto y cualquier otro documento de metadata.


## Creando un Corpus

Primero se debe cargar la librería:

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
library(quanteda)
```

### Construyendo un corpus de un vector de tipo character

El caso más simple sería crear un corpus de un vector de textos que ya estén en la memoria en R. De esta manera, el programador de R obtiene completa flexibilidad con su elección de textos dado que hay virtualmente infinitas posibilidades de obtener un vector de textos en R.

Si ya se disponen de textos en este formato es posible llamar a la función de constructor de corpus directamente. Es posible demostrarlo en el objeto de tipo character integrado de los textos sobre políticas de inmigración extraídos de los manifiestos de partidos políticos compitiendo en la elección del Reino Unido en 2010 (llamado `data_char_ukimmig2010`).

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
corp_uk <- corpus(data_char_ukimmig2010) # construir un nuevo corpus de los textos
summary(corp_uk)
```


Si quisiéramos, también podríamos incorporar también a este corpus algunas variables a nivel documento – lo que quanteda llama `docvars`.

Esto lo hacemos utilizando la función de R llamada `names()` para obtener los nombres del vector de tipo character de `data_char_ukimmig2010` y asignárselos a una variable de documento (`docvar`).

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
docvars(corp_uk, "Party") <- names(data_char_ukimmig2010)
docvars(corp_uk, "Year") <- 2010
summary(corp_uk)
```


### Cargando archivos usando el paquete readtext

```{r, eval=FALSE, echo=TRUE, fig.align='center'}
require(readtext)

# Twitter json
dat_json <- readtext("~/Dropbox/QUANTESS/social media/zombies/tweets.json")
corp_twitter <- corpus(dat_json)
summary(corp_twitter, 5)


# JSON Generico - necesita un especificador de campo de texto
dat_sotu <- readtext("~/Dropbox/QUANTESS/Manuscripts/collocations/Corpora/sotu/sotu.json",
                  textfield = "text")
summary(corpus(dat_sotu), 5)

# Archivo de texto
dat_txtone <- readtext("~/Dropbox/QUANTESS/corpora/project_gutenberg/pg2701.txt", cache = FALSE)
summary(corpus(dat_txtone), 5)

# Archivos de texto múltiples
dat_txtmultiple1 <- readtext("~/Dropbox/QUANTESS/corpora/inaugural/*.txt", cache = FALSE)
summary(corpus(dat_txtmultiple1), 5)

# Múltiples archivos de texto con docvars de los nombres de archivo
dat_txtmultiple2 <- readtext("~/Dropbox/QUANTESS/corpora/inaugural/*.txt",
                             docvarsfrom = "filenames", sep = "-",
                             docvarnames = c("Year", "President"))
summary(corpus(dat_txtmultiple2), 5)

# datos XML 
dat_xml <- readtext("~/Dropbox/QUANTESS/quanteda_working_files/xmlData/plant_catalog.xml",
                  textfield = "COMMON")
summary(corpus(dat_xml), 5)

# archivos csv
write.csv(data.frame(inaug_speech = texts(data_corpus_inaugural),
                     docvars(data_corpus_inaugural)),
          file = "/tmp/inaug_texts.csv", row.names = FALSE)
dat_csv <- readtext("/tmp/inaug_texts.csv", textfield = "inaug_speech")

summary(corpus(dat_csv), 5)
```

### Cómo funciona un corpus de quanteda: Principios del Corpus

Un corpus está diseñado para ser una “librería” original de documentos que han sido convertidos a formato plano, texto codificado en UTF-8, y guardado junto con meta-data en a nivel de corpus y a nivel de documento. Tenemos un nombre especial para meta-data a nivel de documento: `docvars`. Estas son variables o características que describen atributos de cada documento.

Un corpus está diseñado para ser un contenedor de textos más o menos estático en lo que respecta a su procesamiento y análisis. Esto significa que los textos en el corpus no están disenado para ser cambiados internamente a través de (por ejemplo) limpieza o preprocesamiento, como stemming o removiendo la puntuación. 

Más que nada, los textos pueden ser extraídos del corpus como parte del procesamiento y asignados a objetos nuevos, pero la idea es que los corpus se conserven como una copia de referencia original para que otros análisis, por ejemplo aquellos en que stems y puntuación son necesarios, como analizar un índice, pueden ser realizados sobre el mismo corpus.

Para extraer texto de un corpus, es posible utilizar el extractor llamado `texts()`.

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
texts(data_corpus_inaugural)[2]
```
Para obtener la data resumida de textos de un corpus, se puede llamar al método `summary()` definido para un corpus.

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
library(quanteda.textmodels)
data(data_corpus_irishbudget2010, package = "quanteda.textmodels")
summary(data_corpus_irishbudget2010)
```
Se puede guardar el output del comando summary como un data frame y graficar algunos estadísticos descriptivos con esta información:

```{r, eval=TRUE,  out.width = "90%", echo=TRUE, fig.align='center'}
tokeninfo <- summary(data_corpus_inaugural)
if (require(ggplot2))
    ggplot(data = tokeninfo, aes(x = Year, y = Tokens, group = 1)) +
    geom_line() +
    geom_point() +
    scale_x_continuous(labels = c(seq(1789, 2017, 12)), breaks = seq(1789, 2017, 12)) +
    theme_bw()
```
```{r, eval=TRUE, echo=TRUE, fig.align='center'}
# El discurso inaugural más largo: William Henry Harrison
tokeninfo[which.max(tokeninfo$Tokens), ]
```


### Ejercicios 
  * 1 - `data_char_ukimmig2010` es un vector de caracteres con nombre y consta de secciones de los manifiestos electorales británicos sobre inmigración y asilo, crea e imprime un copus de esto:
  
```{r ejercicio1, exercise=TRUE}
corp_immig <- corpus(____, 
  docvars = data.frame(___ = names(_____)))
print(____)
```
<div id="ejercicio1-hint">
**Hint:** El primer parámetro de **corpus** es el dataframe de inicio, y en docvars coloca el vector con los nombres de data_char_ukimmig2010.
</div>
```{r ejercicio1-solution}
corp_immig <- corpus(data_char_ukimmig2010, 
  docvars = data.frame(party = names(data_char_ukimmig2010)))
print(corp_immig)
```
```{r ejercicio1-check}
grade_this_code()
```

  * 2 - Se tiene el siguiente corpus:
  
```{r, eval=TRUE, echo=TRUE, fig.align='center'}
  corp <- data_corpus_inaugural
```
  
  Puede acceder a variables individuales a nivel de documento utilizando el operador `$`. Por ejemplo usando `corp$Year`, otra forma es usando `docvars`, si deseas extraer elementos individuales de las variables del documento, puede especificarlo con el parámetro `field`:
  
```{r ejercicio2, exercise=TRUE}
docvars(corp, ___ = "___")
```
<div id="ejercicio2-hint">
**Hint:** El segundo parametro es el campo que deseas traer, revisa la documentación.
</div>
```{r ejercicio2-solution}
docvars(corp, field = "Year")
```
```{r ejercicio2-check}
grade_this_code()
```

## Manejo de corpus

### Juntando dos objetos de corpus

El operador `+` provee un método simple para concatenar dos objetos corpus. Si contenían diferentes sets de variables a nivel documento las unirá de manera que no se pierda nada de información. La meta-data a nivel corpus también queda concatenada.

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
corp1 <- corpus(data_corpus_inaugural[1:5])
corp2 <- corpus(data_corpus_inaugural[53:58])
corp3 <- corp1 + corp2
summary(corp3)
```

### Armando subsets dentro de objetos corpus

Hay un método de la función `corpus_subset()` definida por objetos corpus, donde un nuevo corpus puede ser extraído en base a condiciones lógicas aplicadas a `docvars`:

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
summary(corpus_subset(data_corpus_inaugural, Year > 1990))
```

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
summary(corpus_subset(data_corpus_inaugural, President == "Adams"))
```

### Explorando textos de corpus

La función `kwic` (**keywords-in-context**) realiza una búsqueda de una palabra y permite visualizar los contextos en los que aparece:

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
kwic(data_corpus_inaugural, pattern = "terror")
```

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
kwic(data_corpus_inaugural, pattern = "terror", valuetype = "regex")
```

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
kwic(data_corpus_inaugural, pattern = "communist*")
```

En el summary de arriba, las variables **Year** (año) y **President** (presidente) son variables asociadas a cada documento. Es posible acceder a dichas variables con la función `docvars()`

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
head(docvars(data_corpus_inaugural))
```

Más corpora están disponibles en el repositorio [quanteda.corpora](https://github.com/quanteda/quanteda.corpora).

### Ejercicios

  * 1 - Para el siguiente texto devuelve una lista de una palabra clave "secure*", además con el tipo de coincidencia de patrones: "glob" para expresiones comodín de estilo "glob", con la cantidad de palabras de contexto que se mostrarán alrededor de la palabra clave sean 3.
  
```{r ejercicio7, exercise=TRUE}
toks <- tokens(data_corpus_inaugural[1:8])
kwic(toks, pattern = "____", valuetype = "___", window = ___)
```
<div id="ejercicio7-hint">
**Hint:** El segundo parametro es el campo que deseas traer, revisa la documentación.
</div>
```{r ejercicio7-solution}
toks <- tokens(data_corpus_inaugural[1:8])
kwic(toks, pattern = "secure*", valuetype = "glob", window = 3)
```
```{r ejercicio7-check}
grade_this_code()
```

  * 2 - Se tiene el siguiente dataframe:
  
```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
dat <- data.frame(letter_factor = factor(rep(letters[1:3], each = 2)),
                  some_ints = 1L:6L,
                  some_text = paste0("Este es un texto numérico ", 1:6, "."),
                  stringsAsFactors = FALSE,
                  row.names = paste0("fromDf_", 1:6))
dat
```  
 
  * Construye un corpus de este dataframe y después haz un resumen del mismo.

```{r ejercicio3, exercise=TRUE}
corpus(___, text_field = "___",
meta = list(source = "_____")) %>% 
summary()
```
<div id="ejercicio3-hint">
**Hint:** El segundo parametro es el campo que deseas traer, revisa la documentación.
</div>
```{r ejercicio3-solution}
corpus(dat, text_field = "some_text",
meta = list(source = "From a data.frame called mydf.")) %>% 
summary()
```
```{r ejercicio3-check}
grade_this_code()
```
  
## Extraer atributos de corpus

Para realizar análisis estadísticos tales como document `scaling`, es necesario extraer una matriz asociando valores de ciertos atributos con cada documento. En quanteda, se utiliza la función `dfm` para producir dicha matriz. `dfm`, por sus siglas en inglés **document-feature matrix** o matriz documento-atributo en español, siempre se refiere a los documentos como filas y a los atributos como columnas.

Se determinó esta orientación de las dimensiones dado que es estándar en el campo de análisis de datos que las unidades de análisis se calculan en las filas y los atributos o variables se calculan en las columnas.

Se denominan **atributos** en vez de términos porque los atributos son más generales que los términos: pueden ser definidos como términos crudos, `términos stemmed`, términos de partes de discurso, términos luego de la remoción de las **stopwords** o una clase de diccionario al que pertenece un término. Los atributos pueden ser enteramente generales, como `ngrams` o dependencias sintácticas y dejamos esto abierto.

### Convirtiendo textos en tokens

Para convertir un texto en tokens de manera simple, quanteda provee un poderoso comando denominado `tokens()`. Produce un objeto intermedio que consiste en una lista de tokens en forma de vectores de caracteres, donde cada elemento de la lista corresponde con un documento de input.

El comando `tokens()` es deliberadamente conservador, es decir, que no remueve nada del texto excepto que se le especifique explícitamente que lo haga.

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
txt <- c(text1 = "Esto es $ 10 en 999 formas diferentes,\n arriba y abajo; ¡izquierda y derecha!", text2 = "@kenbenoit trabajando: en #quanteda 2day\t4ever, http://textasdata.com?page=123.")
tokens(txt)
```
```{r, eval=TRUE, echo=TRUE, fig.align='center'}
tokens(txt, remove_numbers = TRUE,  remove_punct = TRUE)
```

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
tokens(txt, remove_numbers = FALSE, remove_punct = TRUE)
```

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
tokens(txt, remove_numbers = FALSE, remove_punct = FALSE)
```

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
tokens(txt, remove_numbers = FALSE, remove_punct = FALSE, remove_separators = FALSE)
```

También existe la opción de convertir en token los caracteres:

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
tokens("Great website: http://textasdata.com?page=123.", what = "character")
```
```{r, eval=TRUE, echo=TRUE, fig.align='center'}
tokens("Great website: http://textasdata.com?page=123.", what = "character",
        remove_separators = FALSE)
```

y las oraciones:

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
# sentence level         
tokens(c("dijo Kurt Vongeut; solo los idiotas usan punto y coma.",
           "Hoy es jueves en Canberra: es ayer en Londres",
           "En el caso de que no puedas ir con ellos, ¿quieres ir con nosotros?"),
          what = "sentence")
```
### Construyendo una matriz de documentos y atributos

Convertir los textos en tokens es una opción intermedia y la mayoría de los usuarios querrán directamente construir la matriz de documentos y atributos. Para hacer esto existe la función de navaja suiza llamada `dfm()`, que realiza la tokenización y tabula los atributos extraídos dentro de una matriz de documentos por atributos. 

A diferencia del enfoque conservador de `tokens()`, la función `dfm()` aplica ciertas opciones por default, como `tolower()` – una función separada para transformar textos a minúsculas – y remueve puntuación. De todos modos, todas las opciones de `tokens()` se pueden pasar a `dfm()`.

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
corp_inaug_post1990 <- corpus_subset(data_corpus_inaugural, Year > 1990)

# hacer un dfm
dfmat_inaug_post1990 <- tokens(corp_inaug_post1990) %>%
    dfm()
dfmat_inaug_post1990[, 1:5]
```

Otras opciones para incluyen remover las stopwords y realizar stemming de los tokens.

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
# hacer un dfm, eliminando palabras vacías y aplicando derivaciones
dfmat_inaug_post1990 <- dfm(dfmat_inaug_post1990,
                            remove = stopwords("english"),
                            stem = TRUE, remove_punct = TRUE)
```
```{r, eval=TRUE, echo=TRUE, fig.align='center'}
dfmat_inaug_post1990[, 1:5]
```

La opción `remove` provee una lista de tokens a ser ignorados. La mayoría de los usuarios proveerán una lista de `stop words` predefinidas para varios idiomas, accediendo a través de la función `stopwords()`:

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
head(stopwords("en"), 20)

head(stopwords("es"), 20)

head(stopwords("ru"), 10)

head(stopwords("ar", source = "misc"), 10)
```

### Visualizando la matriz de documentos y atributos

  El `dfm` puede ser inspeccionado en el panel de **Environment** en Rstudio o llamando la función View en R. Llamando la función plot en un `dfm` se presentará una nube de palabras usando el paquete wordcloud package

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
dfmat_uk <- dfm(data_char_ukimmig2010, remove = stopwords("english"), remove_punct = TRUE)
dfmat_uk
```

Para acceder a la lista de los atributos más frecuentes es posible utilizar `topfeatures()`:

```{r, eval=TRUE, echo=TRUE, fig.align='center'}
topfeatures(dfmat_uk, 20) # 20 palabras más frecuentes
```

Para un objeto `dfm` se puede graficar una nube de palabras usando `textplot_wordcloud()`.

Esta función pasa argumentos a `wordcloud()` del paquete wordcloud y puede embellecer el gráfico usando los mismos argumentos:

```{r, eval=TRUE, echo=TRUE,  out.width = "90%", fig.align='center', warning=FALSE}
set.seed(100)
library("quanteda.textplots")
textplot_wordcloud(dfmat_uk, min_freq = 6, random_order = FALSE,
                  rotation = .25,
                  colors = RColorBrewer::brewer.pal(8, "Dark2"))
```

### Agrupando palabras por diccionario o clase de equivalencia

Para algunas aplicaciones se tiene conocimiento previo del conjunto de palabras que son indicativas de rasgos que quisiéramos medir. Por ejemplo, una lista general de palabras positivas puede indicar sentimiento positivo en un reseña de una película tal tenemos un diccionario de términos políticos asociados a una tendencia ideológica en particular. 

En estos casos, a veces es útil tratar estos grupos de palabras como equivalentes para los propósitos del análisis y sumar las veces en que se utiliza agregándolas por clase.

Por ejemplo, observemos cómo palabras asociadas al terrorismo y palabras asociadas con la economía varían por presidente en el corpus de discursos inaugurales de presidentes de Estados Unidos. Del corpus original seleccionamos los presidentes desde Clinton:

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
corp_inaug_post1991 <- corpus_subset(data_corpus_inaugural, Year > 1991)
```

Ahora definimos un diccionario de muestra:

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
dict <- dictionary(list(terror = c("terrorism", "terrorists", "threat"),
                        economy = c("jobs", "business", "grow", "work")))
```

Se puede usar el diccionario cuando creamos el dfm:

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
dfmat_inaug_post1991_dict <- dfm(corp_inaug_post1991, dictionary = dict)

dfmat_inaug_post1991_dict
```

El constructor de la función `dictionary()` también funciona con el formato de dos diccionarios externos comunes: los formatos LIWC y Provalis Research’s Wordstat. Por ejemplo, es posible cargar el LIWC y aplicarlo al corpus de discursos inaugurales de presidentes:

```{r, eval=FALSE, echo=TRUE, fig.align='center', warning=FALSE}
dictliwc <- dictionary(file = "~/Dropbox/QUANTESS/dictionaries/LIWC/LIWC2001_English.dic",
                       format = "LIWC")
dfmat_inaug_subset <- dfm(data_corpus_inaugural[52:58], dictionary = dictliwc)
dfmat_inaug_subset[, 1:10]
```


### Ejercicios 

  * 1 - Se tiene la siguiente keyword-in-context:

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
  kw <- kwic(tokens(data_char_sampletext, remove_separators = FALSE),
           pattern = "econom*", separator = "")
```
 
  * ¿Cómo lo podrías transformar a un `corpus` y posteriormente haz un resumen?
 
```{r ejercicio4, exercise=TRUE}
corpus(___, split_context = ___) %>% 
  summary()
```
<div id="ejercicio4-hint">
**Hint:** Inserta el primer parámetro con un dataframe y revisa la documentación para el segundo parámetro.
</div>
```{r ejercicio4-solution}
corpus(kw, split_context = FALSE) %>% 
  summary()
```
```{r ejercicio4-check}
grade_this_code()
```
  
  * 2 - Se tiene el siguiente documento tokenizado:
  
```{r, eval=FALSE, echo=TRUE, fig.align='center', warning=FALSE}
set.seed(123)
toks <- tokens(data_corpus_inaugural[1:6])
toks
```
  * Toma una muestra aleatoria de documentos del tamaño especificado del corpus de tamaño 10 con remplazo:

```{r ejercicio5, exercise=TRUE}
tokens_sample(____, size = ____, replace = ____)
```
<div id="ejercicio5-hint">
**Hint:** Revisa la documentación de **tokens_sample**.
</div>
```{r ejercicio5-solution}
tokens_sample(toks, size = 3, replace = TRUE)
```
```{r ejercicio5-check}
grade_this_code()
```
  
  * 3 - Construye una matriz de características de documento dispersa, a partir del siguiente corpus y filtra después del año **1980**:
 
```{r ejercicio6, exercise=TRUE}
data_corpus_inaugural %>%
  corpus_subset(____) %>%
  tokens() %>% 
  dfm()
```
<div id="ejercicio6-hint">
**Hint:** Filtra por la variable **Year**.
</div>
```{r ejercicio6-solution}
data_corpus_inaugural %>%
  corpus_subset(Year > 1980) %>%
  tokens() %>% 
  dfm()
```
```{r ejercicio6-check}
grade_this_code()
```

## Análisis Típicos

### Similitudes entre textos

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
dfmat_inaug_post1980 <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980),
          remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
```

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
library("quanteda.textstats")
tstat_obama <- textstat_simil(dfmat_inaug_post1980,
                              dfmat_inaug_post1980[c("2009-Obama", "2013-Obama"), ],
                              margin = "documents", method = "cosine")
tstat_obama
```

Se puede utilizar estas distancias para graficar un dendrograma, armando clusters por presidente:

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
data_corpus_sotu <- read_rds("data_corpus_sotu.rds")

dfmat_sotu <- dfm(corpus_subset(data_corpus_sotu, Date > as.Date("1980-01-01")),
               stem = TRUE, remove_punct = TRUE,
               remove = stopwords("english"))
```

```{r, eval=TRUE, echo=TRUE,  out.width = "90%", fig.align='center', warning=FALSE}
dfmat_sotu <- dfm_trim(dfmat_sotu, min_termfreq = 5, min_docfreq = 3)

# agrupamiento jerárquico: obtenga distancias en dfm normalizado
tstat_dist <- textstat_dist(dfm_weight(dfmat_sotu, scheme = "prop"))

# agrupamiento jerárquico del objeto de distancia
pres_cluster <- hclust(as.dist(tstat_dist))

# etiqueta con nombres de documentos
pres_cluster$labels <- docnames(dfmat_sotu)

# trazar como un dendrograma
plot(pres_cluster, xlab = "", sub = "",
     main = "Distancia euclidiana en frecuencia de token normalizada")
```
También se puede observar similitudes de los términos:

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
tstat_sim <- textstat_simil(dfmat_sotu, dfmat_sotu[, c("fair", "health", "terror")],
                          method = "cosine", margin = "features")
lapply(as.list(tstat_sim), head, 10)
```

### Escalamiento de posiciones de documentos

Aquí realizamos una demostración de escalamiento de documentos unsupervised comparado con el modelo **wordfish**:

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
library("quanteda.textmodels")
dfmat_ire <- dfm(data_corpus_irishbudget2010)

tmod_wf <- textmodel_wordfish(dfmat_ire, dir = c(2, 1))

# trazar las estimaciones de Wordfish por partido
textplot_scale1d(tmod_wf, groups = docvars(dfmat_ire, "party"))
```

quanteda hace muy sencillo ajustar topic models también. Por ejemplo:

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
dfmat_ire2 <- dfm(data_corpus_irishbudget2010,
      remove_punct = TRUE, remove_numbers = TRUE, remove = stopwords("english"))
```

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
dfmat_ire2 <- dfm_trim(dfmat_ire2, min_termfreq = 4, max_docfreq = 10)
dfmat_ire2
```

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
set.seed(100)
if (require(topicmodels)) {
    my_lda_fit20 <- LDA(convert(dfmat_ire2, to = "topicmodels"), k = 20)
    get_terms(my_lda_fit20, 5)
}
```

### Visualizacion de Textos

Construir la matriz de co-ocurrencia de características

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
examplefcm <- tokens(data_corpus_irishbudget2010, remove_punct = TRUE) %>%

tokens_tolower() %>%
tokens_remove(stopwords("english"), padding = FALSE) %>%
fcm(context = "window", window = 5, tri = FALSE)
```

Elije 30 características de más frecuencia 

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
topfeats <- names(topfeatures(examplefcm, 30))
```

Seleccione solo las 30 características principales, trace la red

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
set.seed(100)
textplot_network(fcm_select(examplefcm, topfeats), min_freq = 0.8)
```

### Ejercicios 
  * 1 - Se tiene la siguiente matriz
  
```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
set.seed(10)
dfmat1 <- dfm(corpus_subset(data_corpus_inaugural, President == "Obama"),
remove = stopwords("english"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
```

¿Cómo harías una nube de palabras para que a menor frecuencia cambie de color? 

```{r ejercicio9, exercise=TRUE}
textplot_wordcloud(____, rotation = 0.25, 
color = rev(RColorBrewer::brewer.pal(___, ____)))
```
<div id="ejercicio9-hint">
**Hint:** El segundo parametro es el campo que deseas traer, revisa la documentación.
</div>
```{r ejercicio9-solution}
textplot_wordcloud(dfmat1, rotation = 0.25, 
color = rev(RColorBrewer::brewer.pal(10, "RdBu")))
```
```{r ejercicio9-check}
grade_this_code()
```


  * 2 - 
  
## Análisis Estadístico 


### Analisis de Frecuencia Simple

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
corp_tweets <- read_rds("data_corpus_sampletweets.rds")
```

Analizamos los hashtags más frecuentes aplicando `tokens_keep(pattern = "# *")` antes de crear un DFM.

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
toks_tweets <- tokens(corp_tweets, remove_punct = TRUE) %>% 
               tokens_keep(pattern = "#*")
dfmat_tweets <- dfm(toks_tweets)

tstat_freq <- textstat_frequency(dfmat_tweets, n = 5, groups = lang)
head(tstat_freq, 20)
```

También puede trazar las frecuencias de los hashtags de Twitter fácilmente usando `ggplot()`.

```{r, eval=TRUE, echo=TRUE,   out.width = "90%", fig.align='center', warning=FALSE}
dfmat_tweets %>% 
  textstat_frequency(n = 15) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_point() +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()
```

Alternativamente, puede crear una nube de palabras con los 100 hashtags más comunes.

```{r, eval=TRUE, echo=TRUE,   out.width = "90%", fig.align='center', warning=FALSE}
set.seed(132)
textplot_wordcloud(dfmat_tweets, max_words = 100)
```

Finalmente, es posible comparar diferentes grupos dentro de un Wordcloud. Primero creamos una variable ficticia que indica si un tweet se publicó en inglés o en otro idioma. Luego, comparamos los hashtags más frecuentes de tweets en inglés y en otros idiomas.

```{r, eval=TRUE, echo=TRUE,  out.width = "90%", fig.align='center', warning=FALSE}
corp_tweets$dummy_english <- factor(ifelse(corp_tweets$lang == "English", "English", "Not English"))

# tokenizar textos
toks_tweets <- tokens(corp_tweets)

# crear un dfm agrupado y comparar grupos
dfmat_corp_language <- dfm(toks_tweets) %>% 
                       dfm_keep(pattern = "#*") %>% 
                       dfm_group(groups = dummy_english)

# crear nube de palabras
set.seed(132) # set seed for reproducibility
textplot_wordcloud(dfmat_corp_language, comparison = TRUE, max_words = 200)
```

### Diversidad Lexica

`textstat_lexdiv()` calcula varias medidas de diversidad léxica en función del número de tipos únicos de tokens y la longitud de un documento. Es útil, por ejemplo, para analizar las habilidades lingüísticas de los hablantes o escritores, o la complejidad de las ideas expresadas en documentos.

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
toks_inaug <- tokens(data_corpus_inaugural)
dfmat_inaug <- dfm(toks_inaug, remove = stopwords("en"))
```

```{r, eval=TRUE, echo=TRUE, fig.align='center', warning=FALSE}
tstat_lexdiv <- textstat_lexdiv(dfmat_inaug)
tail(tstat_lexdiv, 5)
```

```{r, eval=TRUE, echo=TRUE,  out.width = "90%", fig.align='center', warning=FALSE}
plot(tstat_lexdiv$TTR, type = "l", xaxt = "n", xlab = NULL, ylab = "TTR")
grid()
axis(1, at = seq_len(nrow(tstat_lexdiv)), labels = dfmat_inaug$President)
```
### Análisis de Frecuencias Relativas (keyness)

Las palabras clave es una puntuación de asociación firmada de dos por dos implementada originalmente en WordSmith para identificar palabras frecuentes en documentos en un grupo objetivo y de referencia.

Calcular la keyness, una puntuación para las características que ocurren de manera diferente en las diferentes categorías. Aquí, las categorías se definen por referencia a un índice de documento "tarjet" en el dfm, con el grupo de referencia formado por todos los demás documentos.


```{r, eval=TRUE, echo=TRUE,  out.width = "90%", fig.align='center', warning=FALSE}
# comparar términos anteriores a la posguerra utilizando agrupaciones
period <- ifelse(docvars(data_corpus_inaugural, "Year") < 1945, "pre-war", "post-war")
dfmat1 <- dfm(data_corpus_inaugural, groups = period)
head(dfmat1) # asegúrese de que 'posguerra' esté en la primera fila
```

Con `textstat_keyness()`, puedes comparar frecuencias de palabras entre documentos de referencia y de destino. En este ejemplo, los documentos de destino son artículos de noticias publicados en 2016 y los documentos de referencia son los publicados en 2012-2015. Usamos el paquete **lubridate** para recuperar el año de publicación de un artículo.

```{r, eval=TRUE, echo=TRUE,  out.width = "90%", fig.align='center', warning=FALSE}
require(quanteda.textplots)

dfmat2 <- dfm(data_corpus_inaugural)
head(textstat_keyness(dfmat2, docvars(data_corpus_inaugural, "Year") >= 1945), 10)


dfmat3 <- dfm(corpus_subset(data_corpus_inaugural, period == "post-war"))

textstat_keyness(dfmat3, target = "2017-Trump") %>% 
  textplot_keyness()

```

### Ejercicios 
  * 1 - 
  * 2 - 

## Realizar ejercicios

### Material de referencia

Hadley Wickham y Garrett Grolemund, "R for Data Science: Import, Tidy, Transform, Visualize, and Model Data" O'REILLY [`R for Data Science`](https://r4ds.had.co.nz/)

Enviar hash al siguiente link -> [__Aqui__](https://docs.google.com/spreadsheets/d/1eXNEGrumywfCt4xop5ygcJGxbXI5kfNdiHOztiGspoA/edit?usp=sharing)


